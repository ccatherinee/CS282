"""
This code was taken mostly directly from https://imitation.readthedocs.io/en/latest/algorithms/gail.html and adapted for our purposes.

The original GAIL paper describes the following main components of their algorithm:
1) input expert trajectories, an initial policy, and discriminator parameters
2) train by:
 i) sampling trajectories from the input expert trajectories
 ii) compute a discriminator update
 iii) compute a policy update

Below, we will indicate which parts corresponds to which of these steps.
"""

import numpy as np
from imitation.policies.serialize import load_policy
from imitation.util.util import make_vec_env
from imitation.data.wrappers import RolloutInfoWrapper

SEED = 42

# env is a vectorized version of the MountainCar environment that tracks 8 instances simultaneously
# the number of instances can be changed by setting n_envs, and n_envs=1 is equivalent to loading MountainCar directly from gymnasium
# we will use this to generate expert trajectories
env = make_vec_env(
    "seals:seals/MountainCar-v0",
    rng=np.random.default_rng(SEED),
    n_envs=8,
    post_wrappers=[
        lambda env, _: RolloutInfoWrapper(env)
    ],  # needed for computing rollouts later
)

# expert is a pre-trained model loaded from HuggingFace
# if we wanted to train our own expert, we could use Q-learning like in our previous checkpoint
# we will use this to generate expert trajectories
expert = load_policy(
    "ppo-huggingface",
    env_name="seals:seals/MountainCar-v0",
    venv=env,
)

from imitation.data import rollout

# rollouts is a list of generated trajectories from the expert policy with the given environment
# these are the expert demonstrations that we will use for inverse reinforcement learning
# this forms the "expert trajectories" part of the input to GAIL
rollouts = rollout.rollout(
    expert,
    env,
    rollout.make_sample_until(min_timesteps=None, min_episodes=10),
    rng=np.random.default_rng(SEED),
)

"""
import pickle
# rollouts contains 64 trajectories, each of which is a TrajectoryWithRew
# importantly, TrajectoryWithRew has acts and obs, actions -> acts, obs -> observations

#continuous trajectories
cont_trajectories = [list(zip([tuple(state) for state in t.obs.tolist()[:200]], t.acts)) for t in rollouts]

with open("continuous","wb") as fp:
    pickle.dump(cont_trajectories, fp)
    
# density affects how fine the discretization is, higher density -> finer discetization with more states
def discretize(state, density):
    temp = (state - env.observation_space.low) * np.array([10*density,100*density])
    return tuple(np.round(temp, 0).astype(int))

#discretized trajectories, coarse has fewer states -- 285 total, fine has more states -- 7185
disc_coarse_trajectories = [list(zip([discretize(state,1) for state in t.obs.tolist()[:200]], t.acts)) for t in rollouts]

with open("disc_coarse","wb") as fp:
    pickle.dump(disc_coarse_trajectories, fp)
    
disc_fine_trajectories = [list(zip([discretize(state,5) for state in t.obs.tolist()[:200]], t.acts)) for t in rollouts]

with open("disc_fine","wb") as fp:
    pickle.dump(disc_fine_trajectories, fp)
"""

from imitation.algorithms.adversarial.gail import GAIL
from imitation.rewards.reward_nets import BasicRewardNet
from imitation.util.networks import RunningNorm
from stable_baselines3 import PPO
from stable_baselines3.ppo import MlpPolicy
from stable_baselines3.common.evaluation import evaluate_policy

# This is a learner that uses the proximal policy optimization algorithm
# this is what we will use to compute the policy update steps of the GAIL algorithm
learner = PPO(
    env=env,
    policy=MlpPolicy,
    batch_size=64,
    ent_coef=0.0,
    learning_rate=0.0004,
    gamma=0.95,
    n_epochs=5,
    seed=SEED,
)

# This is a neural network that takes a batch of (state, action, next_state) triples and calculates the associated rewards
# this acts as the discriminator used in GAIL.
# This is what we will use to compute the discriminator update steps of the GAIL algorithm
# according to the original paper: "The job of D is to distinguish between the distribution of data generated by G and the true
# data distribution. When D cannot distinguish data generated by G from the true data, then G has
# successfully matched the true data."
reward_net = BasicRewardNet(
    observation_space=env.observation_space,
    action_space=env.action_space,
    normalize_input_layer=RunningNorm,
)

# imitation implementation of GAIL 
gail_trainer = GAIL(
    demonstrations=rollouts,
    demo_batch_size=1024,
    gen_replay_buffer_capacity=512,
    n_disc_updates_per_round=8,
    venv=env,
    gen_algo=learner,
    reward_net=reward_net,
)

gail_trainer.train(800_000)

# evaluate the learner before we have done any training to get a baseline estimate of performance
# env.seed(SEED)
# learner_rewards_before_training, _ = evaluate_policy(
#     learner, env, 100, return_episode_rewards=True
# )

# train the learner by making 800000 GAIL steps
# gail_trainer.train(800_000)

"""
test_env = make_vec_env(
    "seals:seals/MountainCar-v0",
    rng=np.random.default_rng(SEED),
    n_envs=8,
    post_wrappers=[
        lambda env, _: RolloutInfoWrapper(env)
    ],  # needed for computing rollouts later
)

def reward_summary(rewards):
    mean_reward = np.mean(rewards)
    std_reward = np.std(rewards)
    min_reward = np.min(rewards)
    max_reward = np.max(rewards)
    return (mean_reward, std_reward, min_reward, max_reward)

rewards_data = []
for i in range(100):
    gail_trainer.train(16384)
    test_env.seed(SEED)
    learner_rewards_after_training, _ = evaluate_policy(
        learner, test_env, 100, return_episode_rewards=True
    )
    rewards_data.append(reward_summary(learner_rewards_after_training))

# evaluate the learner after training to determine how much performance has improved
# test_env.seed(SEED)
# learner_rewards_after_training, _ = evaluate_policy(
#     learner, test_env, 100, return_episode_rewards=True
# )

#performance of expert
(expert_mean, expert_std, expert_min, expert_max) = reward_summary([sum(r.rews) for r in rollouts])
"""
